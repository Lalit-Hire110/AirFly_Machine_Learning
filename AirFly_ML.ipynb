{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da2cf70-2c09-4098-9728-673c9958aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c67b345-cb14-4c13-b038-7c83aafc3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (267329, 37)\n",
      "After filtering: (267329, 37)\n",
      "Train / Test sizes: (213863, 37) (53466, 37)\n",
      "After rolling features: train cols = 53 test cols = 53\n",
      "Numeric features used: 28\n",
      "Categorical features used: 4\n",
      "Normal train samples: 191162 Heavy train samples: 22701\n",
      "Training heavy-delay classifier...\n",
      "Classifier metrics (test): acc=0.924 f1=0.679\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.928     0.988     0.957     45709\n",
      "           1      0.885     0.551     0.679      7757\n",
      "\n",
      "    accuracy                          0.924     53466\n",
      "   macro avg      0.907     0.769     0.818     53466\n",
      "weighted avg      0.922     0.924     0.917     53466\n",
      "\n",
      "Confusion matrix:\n",
      " [[45154   555]\n",
      " [ 3485  4272]]\n",
      "XGBoost available: 3.0.5\n",
      "[normal] Running RandomizedSearchCV (n_iter=12) ...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[normal] Best params: {'reg__subsample': 0.8, 'reg__n_estimators': 400, 'reg__max_depth': 5, 'reg__learning_rate': 0.05, 'reg__colsample_bytree': 1.0}\n",
      "[heavy] Running RandomizedSearchCV (n_iter=8) ...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[heavy] Best params: {'reg__subsample': 0.8, 'reg__n_estimators': 400, 'reg__max_depth': 5, 'reg__learning_rate': 0.05, 'reg__colsample_bytree': 1.0}\n",
      "Predicting on test set...\n",
      "\n",
      "Combined model evaluation (test):\n",
      "Samples (test): 53466\n",
      "RMSE : 38.127\n",
      "MAE  : 16.464\n",
      "R²   : 0.606\n",
      "\n",
      "Performance by true-segment:\n",
      " Normal (<=30min): {'count': 45709, 'RMSE': np.float64(15.501137685096841), 'MAE': 12.506446614795259, 'R2': -0.18486709340100949}\n",
      " Heavy  (>30min): {'count': 7757, 'RMSE': np.float64(92.75636098252167), 'MAE': 39.78754741716921, 'R2': 0.40913257242222056}\n",
      "\n",
      "Classifier confusion matrix (rows=true, cols=pred):\n",
      "[[45154   555]\n",
      " [ 3485  4272]]\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Single cell: Restored pipeline (top-N -> group mean/count -> rolling -> models)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ---------- load df ----------\n",
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    candidates = [\"data.csv\", \"./data.csv\", \"/mnt/data/data.csv\"]\n",
    "    loaded = False\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            df = pd.read_csv(p)\n",
    "            loaded = True\n",
    "            print(\"Loaded:\", p)\n",
    "            break\n",
    "    if not loaded:\n",
    "        raise FileNotFoundError(\"No `df` and no data.csv found. Place file or define `df`.\")\n",
    "\n",
    "df = df.copy()\n",
    "print(\"Initial shape:\", df.shape)\n",
    "\n",
    "# ---------- basic cleaning ----------\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df = df[(df['CANCELLED'].astype(float) == 0)]\n",
    "if 'DIVERTED' in df.columns:\n",
    "    df = df[df['DIVERTED'].astype(float) == 0]\n",
    "\n",
    "TARGET = 'ArrivalDelay'\n",
    "df[TARGET] = pd.to_numeric(df[TARGET], errors='coerce')\n",
    "df = df.dropna(subset=[TARGET, 'Date']).reset_index(drop=True)\n",
    "print(\"After filtering:\", df.shape)\n",
    "\n",
    "# ---------- train/test time split ----------\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "split_idx = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_idx].copy()\n",
    "test_df  = df.iloc[split_idx:].copy()\n",
    "print(\"Train / Test sizes:\", train_df.shape, test_df.shape)\n",
    "\n",
    "# ---------- top-N cardinality reduction (do this BEFORE group stats & rolling) ----------\n",
    "def top_n_map(series, n=30):\n",
    "    top = series.value_counts().nlargest(n).index\n",
    "    return series.where(series.isin(top), other='OTHER')\n",
    "\n",
    "# Ensure columns exist and are string typed before mapping\n",
    "for c, n in [('Airline',10), ('ORIGIN',20), ('DEST',20), ('Route',50)]:\n",
    "    if c in train_df.columns:\n",
    "        train_df[c] = train_df[c].astype(str)\n",
    "        test_df[c]  = test_df[c].astype(str)\n",
    "        train_df[c] = top_n_map(train_df[c], n=n)\n",
    "        test_df[c]  = top_n_map(test_df[c], n=n)\n",
    "\n",
    "# ---------- leakage-safe group mean/count features (train-only stats -> map to test) ----------\n",
    "for col in ['Airline', 'ORIGIN', 'DEST', 'Route']:\n",
    "    if col in train_df.columns:\n",
    "        stats = train_df.groupby(col)[TARGET].agg(['mean','count']).rename(columns={'mean':'stat_mean','count':'stat_count'})\n",
    "        # map to train\n",
    "        train_df[f'{col}_mean'] = train_df[col].map(stats['stat_mean'])\n",
    "        train_df[f'{col}_count'] = train_df[col].map(stats['stat_count'])\n",
    "        # map to test using the same train stats\n",
    "        test_df[f'{col}_mean'] = test_df[col].map(stats['stat_mean'])\n",
    "        test_df[f'{col}_count'] = test_df[col].map(stats['stat_count'])\n",
    "        # fill missing in test (group unseen in train)\n",
    "        test_df[f'{col}_mean'] = test_df[f'{col}_mean'].fillna(train_df[f'{col}_mean'].mean())\n",
    "        test_df[f'{col}_count'] = test_df[f'{col}_count'].fillna(0)\n",
    "\n",
    "# ---------- robust rolling helper (idempotent) ----------\n",
    "def add_train_rolling_features(train, test, group_col, target_col='ArrivalDelay', windows_days=(7,14)):\n",
    "    helper = f\"_date_day_{group_col}\"\n",
    "    # remove helper if present\n",
    "    for d in (train, test):\n",
    "        if helper in d.columns:\n",
    "            d.drop(columns=[helper], inplace=True)\n",
    "    # create helper day column\n",
    "    train[helper] = train['Date'].dt.floor('D')\n",
    "    test[helper]  = test['Date'].dt.floor('D')\n",
    "\n",
    "    # daily aggregation from TRAIN only\n",
    "    daily = (train.groupby([group_col, helper])[target_col]\n",
    "             .agg(['mean', 'count']).reset_index().rename(columns={'mean':'daily_mean','count':'daily_count'}))\n",
    "\n",
    "    blocks = []\n",
    "    for gval, g in daily.groupby(group_col):\n",
    "        g = g.sort_values(helper)\n",
    "        idx = pd.date_range(g[helper].min(), g[helper].max(), freq='D')\n",
    "        mean_s = g.set_index(helper)['daily_mean'].reindex(idx)\n",
    "        count_s = g.set_index(helper)['daily_count'].reindex(idx).fillna(0)\n",
    "        tmp = pd.DataFrame({helper: idx, group_col: gval,\n",
    "                            'daily_mean': mean_s.values, 'daily_count': count_s.values})\n",
    "        for w in windows_days:\n",
    "            tmp[f'{group_col}_rolling_mean_{w}d'] = tmp['daily_mean'].rolling(window=w, min_periods=1).mean()\n",
    "            tmp[f'{group_col}_rolling_count_{w}d'] = tmp['daily_count'].rolling(window=w, min_periods=1).sum()\n",
    "        blocks.append(tmp)\n",
    "    if len(blocks) == 0:\n",
    "        # cleanup and return\n",
    "        train.drop(columns=[helper], inplace=True)\n",
    "        test.drop(columns=[helper], inplace=True)\n",
    "        return train, test\n",
    "\n",
    "    rolling_all = pd.concat(blocks, ignore_index=True)\n",
    "    roll_cols = [c for c in rolling_all.columns if 'rolling_' in c]\n",
    "\n",
    "    # join rolling features into TRAIN by group + day\n",
    "    train = train.merge(rolling_all[[group_col, helper] + roll_cols], on=[group_col, helper], how='left')\n",
    "\n",
    "    # For TEST: map last available rolling values from train (no leakage)\n",
    "    last_vals = rolling_all.sort_values(helper).groupby(group_col).last()\n",
    "    for col in roll_cols:\n",
    "        test[col] = test[group_col].map(last_vals[col])\n",
    "        # if still missing, fallback to global train mean of that rolling column\n",
    "        if col in train.columns:\n",
    "            test[col] = test[col].fillna(train[col].mean())\n",
    "\n",
    "    # cleanup helper\n",
    "    train.drop(columns=[helper], inplace=True)\n",
    "    test.drop(columns=[helper], inplace=True)\n",
    "    return train, test\n",
    "\n",
    "# Apply rolling features (after top-n and group stats)\n",
    "for g in ['Route','Airline']:\n",
    "    if g in train_df.columns:\n",
    "        train_df, test_df = add_train_rolling_features(train_df, test_df, g, TARGET, windows_days=(7,14))\n",
    "\n",
    "print(\"After rolling features: train cols =\", train_df.shape[1], \"test cols =\", test_df.shape[1])\n",
    "\n",
    "# ---------- basic feature engineering ----------\n",
    "num_candidates = ['TAXI_OUT','TAXI_IN','AIR_TIME','DISTANCE','CRS_ELAPSED_TIME','ELAPSED_TIME',\n",
    "                  'DepartureHour','ArrivalHour','ScheduledDep']\n",
    "for c in num_candidates:\n",
    "    if c in train_df.columns:\n",
    "        train_df[c] = pd.to_numeric(train_df[c], errors='coerce')\n",
    "        test_df[c]  = pd.to_numeric(test_df[c], errors='coerce')\n",
    "\n",
    "if 'ScheduledDep' in train_df.columns:\n",
    "    train_df['ScheduledDepHour'] = (train_df['ScheduledDep'] // 100).astype('Int64')\n",
    "    test_df['ScheduledDepHour']  = (test_df['ScheduledDep'] // 100).astype('Int64')\n",
    "\n",
    "for d in (train_df, test_df):\n",
    "    d['month'] = d['Date'].dt.month\n",
    "    d['dayofweek'] = d['Date'].dt.dayofweek\n",
    "    d['is_weekend'] = d['dayofweek'].isin([5,6]).astype(int)\n",
    "\n",
    "# ---------- feature lists ----------\n",
    "base_numeric = ['TAXI_OUT','TAXI_IN','AIR_TIME','DISTANCE','CRS_ELAPSED_TIME','ELAPSED_TIME',\n",
    "                'ScheduledDepHour','DepartureHour','ArrivalHour','month','dayofweek','is_weekend']\n",
    "numeric_features = [c for c in base_numeric if c in train_df.columns]\n",
    "# add rolling cols\n",
    "numeric_features += [c for c in train_df.columns if ('rolling_mean' in c) or ('rolling_count' in c)]\n",
    "# add group mean/count columns if present\n",
    "for suf in ['_mean','_count']:\n",
    "    for grp in ['Airline','ORIGIN','DEST','Route']:\n",
    "        col = f'{grp}{suf}'\n",
    "        if col in train_df.columns:\n",
    "            numeric_features.append(col)\n",
    "\n",
    "categorical_features = [c for c in ['Airline','ORIGIN','DEST','Route'] if c in train_df.columns]\n",
    "\n",
    "print(\"Numeric features used:\", len(numeric_features))\n",
    "print(\"Categorical features used:\", len(categorical_features))\n",
    "\n",
    "# ---------- prepare X/y ----------\n",
    "HEAVY_THRESH = 30.0\n",
    "\n",
    "X_train_full = train_df[numeric_features + categorical_features].copy()\n",
    "X_test_full  = test_df[numeric_features + categorical_features].copy()\n",
    "\n",
    "y_train_class = (train_df[TARGET] > HEAVY_THRESH).astype(int)\n",
    "y_test_class  = (test_df[TARGET] > HEAVY_THRESH).astype(int)\n",
    "\n",
    "train_idx_norm = train_df[train_df[TARGET] <= HEAVY_THRESH].index\n",
    "train_idx_heavy = train_df[train_df[TARGET] > HEAVY_THRESH].index\n",
    "\n",
    "y_norm_s = np.log1p(np.clip(train_df.loc[train_idx_norm, TARGET], a_min=0, a_max=None))\n",
    "y_heavy_s = np.log1p(np.clip(train_df.loc[train_idx_heavy, TARGET], a_min=0, a_max=None))\n",
    "\n",
    "y_norm_s = pd.Series(y_norm_s, index=train_idx_norm).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "y_heavy_s = pd.Series(y_heavy_s, index=train_idx_heavy).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "X_train_norm = X_train_full.loc[y_norm_s.index].copy()\n",
    "X_train_heavy = X_train_full.loc[y_heavy_s.index].copy()\n",
    "y_train_norm = y_norm_s.values\n",
    "y_train_heavy = y_heavy_s.values\n",
    "\n",
    "print(\"Normal train samples:\", X_train_norm.shape[0], \"Heavy train samples:\", X_train_heavy.shape[0])\n",
    "\n",
    "# ---------- preprocessor ----------\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='MISSING')),\n",
    "    ('ohe', ohe)\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='drop', n_jobs=-1)\n",
    "\n",
    "# ---------- classifier ----------\n",
    "clf_pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', HistGradientBoostingClassifier(random_state=RANDOM_SEED))\n",
    "])\n",
    "print(\"Training heavy-delay classifier...\")\n",
    "clf_pipe.fit(X_train_full, y_train_class)\n",
    "y_test_pred_class = clf_pipe.predict(X_test_full)\n",
    "print(\"Classifier metrics (test): acc=%.3f f1=%.3f\" % (accuracy_score(y_test_class, y_test_pred_class), f1_score(y_test_class, y_test_pred_class)))\n",
    "print(classification_report(y_test_class, y_test_pred_class, digits=3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test_class, y_test_pred_class))\n",
    "\n",
    "# ---------- regressors (tuning & train) ----------\n",
    "USE_XGB = False\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBRegressor\n",
    "    USE_XGB = True\n",
    "    print(\"XGBoost available:\", xgb.__version__)\n",
    "except Exception:\n",
    "    print(\"XGBoost unavailable; using HistGradientBoostingRegressor fallback.\")\n",
    "    USE_XGB = False\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "def tune_and_train_regressor(X_sub, y_sub, model_name='normal', n_iter=12):\n",
    "    if X_sub.shape[0] < 100:\n",
    "        print(f\"[{model_name}] Too few samples ({X_sub.shape[0]}). Fitting default regressor without tuning.\")\n",
    "        if USE_XGB:\n",
    "            reg = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=RANDOM_SEED, n_jobs=-1, verbosity=0)\n",
    "        else:\n",
    "            reg = HistGradientBoostingRegressor(max_iter=200, random_state=RANDOM_SEED)\n",
    "        pipe = Pipeline([('preproc', preprocessor), ('reg', reg)])\n",
    "        pipe.fit(X_sub, y_sub)\n",
    "        return pipe\n",
    "\n",
    "    if USE_XGB:\n",
    "        base = XGBRegressor(objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1, verbosity=0)\n",
    "        param_dist = {\n",
    "            'reg__n_estimators': [100,200,400],\n",
    "            'reg__max_depth': [3,5,7],\n",
    "            'reg__learning_rate': [0.01,0.03,0.05],\n",
    "            'reg__subsample': [0.6,0.8,1.0],\n",
    "            'reg__colsample_bytree': [0.6,0.8,1.0]\n",
    "        }\n",
    "    else:\n",
    "        base = HistGradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "        param_dist = {\n",
    "            'reg__max_iter': [100,200,400],\n",
    "            'reg__max_leaf_nodes': [15,31,63],\n",
    "            'reg__learning_rate': [0.01,0.05,0.1]\n",
    "        }\n",
    "\n",
    "    pipe = Pipeline([('preproc', preprocessor), ('reg', base)])\n",
    "    rsearch = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=n_iter, cv=tscv,\n",
    "                                 scoring='neg_mean_absolute_error', n_jobs=-1, random_state=RANDOM_SEED, verbose=1, refit=True)\n",
    "    print(f\"[{model_name}] Running RandomizedSearchCV (n_iter={n_iter}) ...\")\n",
    "    rsearch.fit(X_sub, y_sub)\n",
    "    print(f\"[{model_name}] Best params:\", rsearch.best_params_)\n",
    "    return rsearch.best_estimator_\n",
    "\n",
    "normal_reg_pipe = tune_and_train_regressor(X_train_norm, y_train_norm, model_name='normal', n_iter=12)\n",
    "heavy_reg_pipe = tune_and_train_regressor(X_train_heavy, y_train_heavy, model_name='heavy', n_iter=8)\n",
    "\n",
    "# ---------- predict & combine ----------\n",
    "print(\"Predicting on test set...\")\n",
    "clf_pred = clf_pipe.predict(X_test_full)\n",
    "\n",
    "pred_norm_log = normal_reg_pipe.predict(X_test_full)\n",
    "pred_heavy_log = heavy_reg_pipe.predict(X_test_full)\n",
    "\n",
    "pred_norm = np.expm1(pred_norm_log)\n",
    "pred_heavy = np.expm1(pred_heavy_log)\n",
    "\n",
    "pred = np.where(clf_pred == 1, pred_heavy, pred_norm)\n",
    "pred = np.clip(pred, a_min=0.0, a_max=None)\n",
    "\n",
    "# ---------- evaluation ----------\n",
    "y_test = test_df[TARGET].values\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "mae  = mean_absolute_error(y_test, pred)\n",
    "r2   = r2_score(y_test, pred)\n",
    "print(\"\\nCombined model evaluation (test):\")\n",
    "print(f\"Samples (test): {len(y_test)}\")\n",
    "print(f\"RMSE : {rmse:.3f}\")\n",
    "print(f\"MAE  : {mae:.3f}\")\n",
    "print(f\"R²   : {r2:.3f}\")\n",
    "\n",
    "mask_normal = (test_df[TARGET] <= HEAVY_THRESH).values\n",
    "mask_heavy  = (test_df[TARGET] > HEAVY_THRESH).values\n",
    "\n",
    "def seg_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'count': len(y_true),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)) if len(y_true)>0 else np.nan,\n",
    "        'MAE': mean_absolute_error(y_true, y_pred) if len(y_true)>0 else np.nan,\n",
    "        'R2': r2_score(y_true, y_pred) if len(y_true)>1 else np.nan\n",
    "    }\n",
    "\n",
    "print(\"\\nPerformance by true-segment:\")\n",
    "print(\" Normal (<=30min):\", seg_metrics(y_test[mask_normal], pred[mask_normal]))\n",
    "print(\" Heavy  (>30min):\", seg_metrics(y_test[mask_heavy], pred[mask_heavy]))\n",
    "\n",
    "print(\"\\nClassifier confusion matrix (rows=true, cols=pred):\")\n",
    "print(confusion_matrix(y_test_class, clf_pred))\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143deb7-6cb3-4f36-8a83-ce6d37e4075f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
