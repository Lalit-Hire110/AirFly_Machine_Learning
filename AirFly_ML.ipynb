{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da2cf70-2c09-4098-9728-673c9958aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c67b345-cb14-4c13-b038-7c83aafc3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (275272, 37)\n",
      "After filtering: (267329, 37)\n",
      "Train / Test sizes: (213863, 37) (53466, 37)\n",
      "After rolling features: train cols = 53 test cols = 53\n",
      "Numeric features used: 28\n",
      "Categorical features used: 4\n",
      "Normal train samples: 191160 Heavy train samples: 22703\n",
      "Training heavy-delay classifier...\n",
      "Classifier metrics (test): acc=0.927 f1=0.698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.933     0.986     0.959     45711\n",
      "           1      0.874     0.582     0.698      7755\n",
      "\n",
      "    accuracy                          0.927     53466\n",
      "   macro avg      0.903     0.784     0.828     53466\n",
      "weighted avg      0.924     0.927     0.921     53466\n",
      "\n",
      "Confusion matrix:\n",
      " [[45058   653]\n",
      " [ 3245  4510]]\n",
      "XGBoost available: 3.0.5\n",
      "[normal] Running RandomizedSearchCV (n_iter=12) ...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[normal] Best params: {'reg__subsample': 0.8, 'reg__n_estimators': 400, 'reg__max_depth': 5, 'reg__learning_rate': 0.05, 'reg__colsample_bytree': 1.0}\n",
      "[heavy] Running RandomizedSearchCV (n_iter=8) ...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[heavy] Best params: {'reg__subsample': 0.8, 'reg__n_estimators': 400, 'reg__max_depth': 5, 'reg__learning_rate': 0.05, 'reg__colsample_bytree': 1.0}\n",
      "Predicting on test set...\n",
      "\n",
      "Combined model evaluation (test):\n",
      "Samples (test): 53466\n",
      "RMSE : 38.226\n",
      "MAE  : 16.313\n",
      "R²   : 0.604\n",
      "\n",
      "Performance by true-segment:\n",
      " Normal (<=30min): {'count': 45711, 'RMSE': np.float64(15.551691883063254), 'MAE': 12.542897683033189, 'R2': -0.19318295084326875}\n",
      " Heavy  (>30min): {'count': 7755, 'RMSE': np.float64(92.9990984950073), 'MAE': 38.53810799776563, 'R2': 0.4060879538034202}\n",
      "\n",
      "Classifier confusion matrix (rows=true, cols=pred):\n",
      "[[45058   653]\n",
      " [ 3245  4510]]\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Single cell: Restored pipeline (top-N -> group mean/count -> rolling -> models)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ---------- load df ----------\n",
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    candidates = [\"data.csv\", \"./data.csv\", \"/mnt/data/data.csv\"]\n",
    "    loaded = False\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            df = pd.read_csv(p)\n",
    "            loaded = True\n",
    "            print(\"Loaded:\", p)\n",
    "            break\n",
    "    if not loaded:\n",
    "        raise FileNotFoundError(\"No `df` and no data.csv found. Place file or define `df`.\")\n",
    "\n",
    "df = df.copy()\n",
    "print(\"Initial shape:\", df.shape)\n",
    "\n",
    "# ---------- basic cleaning ----------\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df = df[(df['CANCELLED'].astype(float) == 0)]\n",
    "if 'DIVERTED' in df.columns:\n",
    "    df = df[df['DIVERTED'].astype(float) == 0]\n",
    "\n",
    "TARGET = 'ArrivalDelay'\n",
    "df[TARGET] = pd.to_numeric(df[TARGET], errors='coerce')\n",
    "df = df.dropna(subset=[TARGET, 'Date']).reset_index(drop=True)\n",
    "print(\"After filtering:\", df.shape)\n",
    "\n",
    "# ---------- train/test time split ----------\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "split_idx = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_idx].copy()\n",
    "test_df  = df.iloc[split_idx:].copy()\n",
    "print(\"Train / Test sizes:\", train_df.shape, test_df.shape)\n",
    "\n",
    "# ---------- top-N cardinality reduction (do this BEFORE group stats & rolling) ----------\n",
    "def top_n_map(series, n=30):\n",
    "    top = series.value_counts().nlargest(n).index\n",
    "    return series.where(series.isin(top), other='OTHER')\n",
    "\n",
    "# Ensure columns exist and are string typed before mapping\n",
    "for c, n in [('Airline',10), ('ORIGIN',20), ('DEST',20), ('Route',50)]:\n",
    "    if c in train_df.columns:\n",
    "        train_df[c] = train_df[c].astype(str)\n",
    "        test_df[c]  = test_df[c].astype(str)\n",
    "        train_df[c] = top_n_map(train_df[c], n=n)\n",
    "        test_df[c]  = top_n_map(test_df[c], n=n)\n",
    "\n",
    "# ---------- leakage-safe group mean/count features (train-only stats -> map to test) ----------\n",
    "for col in ['Airline', 'ORIGIN', 'DEST', 'Route']:\n",
    "    if col in train_df.columns:\n",
    "        stats = train_df.groupby(col)[TARGET].agg(['mean','count']).rename(columns={'mean':'stat_mean','count':'stat_count'})\n",
    "        # map to train\n",
    "        train_df[f'{col}_mean'] = train_df[col].map(stats['stat_mean'])\n",
    "        train_df[f'{col}_count'] = train_df[col].map(stats['stat_count'])\n",
    "        # map to test using the same train stats\n",
    "        test_df[f'{col}_mean'] = test_df[col].map(stats['stat_mean'])\n",
    "        test_df[f'{col}_count'] = test_df[col].map(stats['stat_count'])\n",
    "        # fill missing in test (group unseen in train)\n",
    "        test_df[f'{col}_mean'] = test_df[f'{col}_mean'].fillna(train_df[f'{col}_mean'].mean())\n",
    "        test_df[f'{col}_count'] = test_df[f'{col}_count'].fillna(0)\n",
    "\n",
    "# ---------- robust rolling helper (idempotent) ----------\n",
    "def add_train_rolling_features(train, test, group_col, target_col='ArrivalDelay', windows_days=(7,14)):\n",
    "    helper = f\"_date_day_{group_col}\"\n",
    "    # remove helper if present\n",
    "    for d in (train, test):\n",
    "        if helper in d.columns:\n",
    "            d.drop(columns=[helper], inplace=True)\n",
    "    # create helper day column\n",
    "    train[helper] = train['Date'].dt.floor('D')\n",
    "    test[helper]  = test['Date'].dt.floor('D')\n",
    "\n",
    "    # daily aggregation from TRAIN only\n",
    "    daily = (train.groupby([group_col, helper])[target_col]\n",
    "             .agg(['mean', 'count']).reset_index().rename(columns={'mean':'daily_mean','count':'daily_count'}))\n",
    "\n",
    "    blocks = []\n",
    "    for gval, g in daily.groupby(group_col):\n",
    "        g = g.sort_values(helper)\n",
    "        idx = pd.date_range(g[helper].min(), g[helper].max(), freq='D')\n",
    "        mean_s = g.set_index(helper)['daily_mean'].reindex(idx)\n",
    "        count_s = g.set_index(helper)['daily_count'].reindex(idx).fillna(0)\n",
    "        tmp = pd.DataFrame({helper: idx, group_col: gval,\n",
    "                            'daily_mean': mean_s.values, 'daily_count': count_s.values})\n",
    "        for w in windows_days:\n",
    "            tmp[f'{group_col}_rolling_mean_{w}d'] = tmp['daily_mean'].rolling(window=w, min_periods=1).mean()\n",
    "            tmp[f'{group_col}_rolling_count_{w}d'] = tmp['daily_count'].rolling(window=w, min_periods=1).sum()\n",
    "        blocks.append(tmp)\n",
    "    if len(blocks) == 0:\n",
    "        # cleanup and return\n",
    "        train.drop(columns=[helper], inplace=True)\n",
    "        test.drop(columns=[helper], inplace=True)\n",
    "        return train, test\n",
    "\n",
    "    rolling_all = pd.concat(blocks, ignore_index=True)\n",
    "    roll_cols = [c for c in rolling_all.columns if 'rolling_' in c]\n",
    "\n",
    "    # join rolling features into TRAIN by group + day\n",
    "    train = train.merge(rolling_all[[group_col, helper] + roll_cols], on=[group_col, helper], how='left')\n",
    "\n",
    "    # For TEST: map last available rolling values from train (no leakage)\n",
    "    last_vals = rolling_all.sort_values(helper).groupby(group_col).last()\n",
    "    for col in roll_cols:\n",
    "        test[col] = test[group_col].map(last_vals[col])\n",
    "        # if still missing, fallback to global train mean of that rolling column\n",
    "        if col in train.columns:\n",
    "            test[col] = test[col].fillna(train[col].mean())\n",
    "\n",
    "    # cleanup helper\n",
    "    train.drop(columns=[helper], inplace=True)\n",
    "    test.drop(columns=[helper], inplace=True)\n",
    "    return train, test\n",
    "\n",
    "# Apply rolling features (after top-n and group stats)\n",
    "for g in ['Route','Airline']:\n",
    "    if g in train_df.columns:\n",
    "        train_df, test_df = add_train_rolling_features(train_df, test_df, g, TARGET, windows_days=(7,14))\n",
    "\n",
    "print(\"After rolling features: train cols =\", train_df.shape[1], \"test cols =\", test_df.shape[1])\n",
    "\n",
    "# ---------- basic feature engineering ----------\n",
    "num_candidates = ['TAXI_OUT','TAXI_IN','AIR_TIME','DISTANCE','CRS_ELAPSED_TIME','ELAPSED_TIME',\n",
    "                  'DepartureHour','ArrivalHour','ScheduledDep']\n",
    "for c in num_candidates:\n",
    "    if c in train_df.columns:\n",
    "        train_df[c] = pd.to_numeric(train_df[c], errors='coerce')\n",
    "        test_df[c]  = pd.to_numeric(test_df[c], errors='coerce')\n",
    "\n",
    "if 'ScheduledDep' in train_df.columns:\n",
    "    train_df['ScheduledDepHour'] = (train_df['ScheduledDep'] // 100).astype('Int64')\n",
    "    test_df['ScheduledDepHour']  = (test_df['ScheduledDep'] // 100).astype('Int64')\n",
    "\n",
    "for d in (train_df, test_df):\n",
    "    d['month'] = d['Date'].dt.month\n",
    "    d['dayofweek'] = d['Date'].dt.dayofweek\n",
    "    d['is_weekend'] = d['dayofweek'].isin([5,6]).astype(int)\n",
    "\n",
    "# ---------- feature lists ----------\n",
    "base_numeric = ['TAXI_OUT','TAXI_IN','AIR_TIME','DISTANCE','CRS_ELAPSED_TIME','ELAPSED_TIME',\n",
    "                'ScheduledDepHour','DepartureHour','ArrivalHour','month','dayofweek','is_weekend']\n",
    "numeric_features = [c for c in base_numeric if c in train_df.columns]\n",
    "# add rolling cols\n",
    "numeric_features += [c for c in train_df.columns if ('rolling_mean' in c) or ('rolling_count' in c)]\n",
    "# add group mean/count columns if present\n",
    "for suf in ['_mean','_count']:\n",
    "    for grp in ['Airline','ORIGIN','DEST','Route']:\n",
    "        col = f'{grp}{suf}'\n",
    "        if col in train_df.columns:\n",
    "            numeric_features.append(col)\n",
    "\n",
    "categorical_features = [c for c in ['Airline','ORIGIN','DEST','Route'] if c in train_df.columns]\n",
    "\n",
    "print(\"Numeric features used:\", len(numeric_features))\n",
    "print(\"Categorical features used:\", len(categorical_features))\n",
    "\n",
    "# ---------- prepare X/y ----------\n",
    "HEAVY_THRESH = 30.0\n",
    "\n",
    "X_train_full = train_df[numeric_features + categorical_features].copy()\n",
    "X_test_full  = test_df[numeric_features + categorical_features].copy()\n",
    "\n",
    "y_train_class = (train_df[TARGET] > HEAVY_THRESH).astype(int)\n",
    "y_test_class  = (test_df[TARGET] > HEAVY_THRESH).astype(int)\n",
    "\n",
    "train_idx_norm = train_df[train_df[TARGET] <= HEAVY_THRESH].index\n",
    "train_idx_heavy = train_df[train_df[TARGET] > HEAVY_THRESH].index\n",
    "\n",
    "y_norm_s = np.log1p(np.clip(train_df.loc[train_idx_norm, TARGET], a_min=0, a_max=None))\n",
    "y_heavy_s = np.log1p(np.clip(train_df.loc[train_idx_heavy, TARGET], a_min=0, a_max=None))\n",
    "\n",
    "y_norm_s = pd.Series(y_norm_s, index=train_idx_norm).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "y_heavy_s = pd.Series(y_heavy_s, index=train_idx_heavy).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "X_train_norm = X_train_full.loc[y_norm_s.index].copy()\n",
    "X_train_heavy = X_train_full.loc[y_heavy_s.index].copy()\n",
    "y_train_norm = y_norm_s.values\n",
    "y_train_heavy = y_heavy_s.values\n",
    "\n",
    "print(\"Normal train samples:\", X_train_norm.shape[0], \"Heavy train samples:\", X_train_heavy.shape[0])\n",
    "\n",
    "# ---------- preprocessor ----------\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='MISSING')),\n",
    "    ('ohe', ohe)\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='drop', n_jobs=-1)\n",
    "\n",
    "# ---------- classifier ----------\n",
    "clf_pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', HistGradientBoostingClassifier(random_state=RANDOM_SEED))\n",
    "])\n",
    "print(\"Training heavy-delay classifier...\")\n",
    "clf_pipe.fit(X_train_full, y_train_class)\n",
    "y_test_pred_class = clf_pipe.predict(X_test_full)\n",
    "print(\"Classifier metrics (test): acc=%.3f f1=%.3f\" % (accuracy_score(y_test_class, y_test_pred_class), f1_score(y_test_class, y_test_pred_class)))\n",
    "print(classification_report(y_test_class, y_test_pred_class, digits=3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test_class, y_test_pred_class))\n",
    "\n",
    "# ---------- regressors (tuning & train) ----------\n",
    "USE_XGB = False\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBRegressor\n",
    "    USE_XGB = True\n",
    "    print(\"XGBoost available:\", xgb.__version__)\n",
    "except Exception:\n",
    "    print(\"XGBoost unavailable; using HistGradientBoostingRegressor fallback.\")\n",
    "    USE_XGB = False\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "def tune_and_train_regressor(X_sub, y_sub, model_name='normal', n_iter=12):\n",
    "    if X_sub.shape[0] < 100:\n",
    "        print(f\"[{model_name}] Too few samples ({X_sub.shape[0]}). Fitting default regressor without tuning.\")\n",
    "        if USE_XGB:\n",
    "            reg = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=RANDOM_SEED, n_jobs=-1, verbosity=0)\n",
    "        else:\n",
    "            reg = HistGradientBoostingRegressor(max_iter=200, random_state=RANDOM_SEED)\n",
    "        pipe = Pipeline([('preproc', preprocessor), ('reg', reg)])\n",
    "        pipe.fit(X_sub, y_sub)\n",
    "        return pipe\n",
    "\n",
    "    if USE_XGB:\n",
    "        base = XGBRegressor(objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1, verbosity=0)\n",
    "        param_dist = {\n",
    "            'reg__n_estimators': [100,200,400],\n",
    "            'reg__max_depth': [3,5,7],\n",
    "            'reg__learning_rate': [0.01,0.03,0.05],\n",
    "            'reg__subsample': [0.6,0.8,1.0],\n",
    "            'reg__colsample_bytree': [0.6,0.8,1.0]\n",
    "        }\n",
    "    else:\n",
    "        base = HistGradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "        param_dist = {\n",
    "            'reg__max_iter': [100,200,400],\n",
    "            'reg__max_leaf_nodes': [15,31,63],\n",
    "            'reg__learning_rate': [0.01,0.05,0.1]\n",
    "        }\n",
    "\n",
    "    pipe = Pipeline([('preproc', preprocessor), ('reg', base)])\n",
    "    rsearch = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=n_iter, cv=tscv,\n",
    "                                 scoring='neg_mean_absolute_error', n_jobs=-1, random_state=RANDOM_SEED, verbose=1, refit=True)\n",
    "    print(f\"[{model_name}] Running RandomizedSearchCV (n_iter={n_iter}) ...\")\n",
    "    rsearch.fit(X_sub, y_sub)\n",
    "    print(f\"[{model_name}] Best params:\", rsearch.best_params_)\n",
    "    return rsearch.best_estimator_\n",
    "\n",
    "normal_reg_pipe = tune_and_train_regressor(X_train_norm, y_train_norm, model_name='normal', n_iter=12)\n",
    "heavy_reg_pipe = tune_and_train_regressor(X_train_heavy, y_train_heavy, model_name='heavy', n_iter=8)\n",
    "\n",
    "# ---------- predict & combine ----------\n",
    "print(\"Predicting on test set...\")\n",
    "clf_pred = clf_pipe.predict(X_test_full)\n",
    "\n",
    "pred_norm_log = normal_reg_pipe.predict(X_test_full)\n",
    "pred_heavy_log = heavy_reg_pipe.predict(X_test_full)\n",
    "\n",
    "pred_norm = np.expm1(pred_norm_log)\n",
    "pred_heavy = np.expm1(pred_heavy_log)\n",
    "\n",
    "pred = np.where(clf_pred == 1, pred_heavy, pred_norm)\n",
    "pred = np.clip(pred, a_min=0.0, a_max=None)\n",
    "\n",
    "# ---------- evaluation ----------\n",
    "y_test = test_df[TARGET].values\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "mae  = mean_absolute_error(y_test, pred)\n",
    "r2   = r2_score(y_test, pred)\n",
    "print(\"\\nCombined model evaluation (test):\")\n",
    "print(f\"Samples (test): {len(y_test)}\")\n",
    "print(f\"RMSE : {rmse:.3f}\")\n",
    "print(f\"MAE  : {mae:.3f}\")\n",
    "print(f\"R²   : {r2:.3f}\")\n",
    "\n",
    "mask_normal = (test_df[TARGET] <= HEAVY_THRESH).values\n",
    "mask_heavy  = (test_df[TARGET] > HEAVY_THRESH).values\n",
    "\n",
    "def seg_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'count': len(y_true),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)) if len(y_true)>0 else np.nan,\n",
    "        'MAE': mean_absolute_error(y_true, y_pred) if len(y_true)>0 else np.nan,\n",
    "        'R2': r2_score(y_true, y_pred) if len(y_true)>1 else np.nan\n",
    "    }\n",
    "\n",
    "print(\"\\nPerformance by true-segment:\")\n",
    "print(\" Normal (<=30min):\", seg_metrics(y_test[mask_normal], pred[mask_normal]))\n",
    "print(\" Heavy  (>30min):\", seg_metrics(y_test[mask_heavy], pred[mask_heavy]))\n",
    "\n",
    "print(\"\\nClassifier confusion matrix (rows=true, cols=pred):\")\n",
    "print(confusion_matrix(y_test_class, clf_pred))\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a5a72-6a9b-4f5d-8847-dcea3ed31cd5",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984c6106-e31d-44ff-bde6-d080608c8974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving pipelines...\n",
      "Saving feature metadata...\n",
      "Creating predictor helper module...\n",
      "Saved artifacts to: model_artifacts\n",
      "Artifacts list: [WindowsPath('model_artifacts/clf_pipe.joblib'), WindowsPath('model_artifacts/features.json'), WindowsPath('model_artifacts/heavy_reg_pipe.joblib'), WindowsPath('model_artifacts/metadata.json'), WindowsPath('model_artifacts/normal_reg_pipe.joblib'), WindowsPath('model_artifacts/predictor.py'), WindowsPath('model_artifacts/versions.json'), WindowsPath('model_artifacts/__pycache__')]\n",
      "Smoke test prediction (example): {}\n"
     ]
    }
   ],
   "source": [
    "# Save models + artifacts (single cell)\n",
    "import os, json, sys\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import platform\n",
    "\n",
    "# required objects (should exist in notebook)\n",
    "# clf_pipe, normal_reg_pipe, heavy_reg_pipe, numeric_features, categorical_features, HEAVY_THRESH, TARGET\n",
    "# optional: test_df for sample inference\n",
    "\n",
    "ARTIFACT_DIR = Path(\"model_artifacts\")\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# 1) save trained pipelines\n",
    "print(\"Saving pipelines...\")\n",
    "dump(clf_pipe, ARTIFACT_DIR / \"clf_pipe.joblib\")\n",
    "dump(normal_reg_pipe, ARTIFACT_DIR / \"normal_reg_pipe.joblib\")\n",
    "dump(heavy_reg_pipe, ARTIFACT_DIR / \"heavy_reg_pipe.joblib\")\n",
    "\n",
    "# 2) save feature lists & constants\n",
    "print(\"Saving feature metadata...\")\n",
    "meta = {\n",
    "    \"numeric_features\": list(map(str, numeric_features)),\n",
    "    \"categorical_features\": list(map(str, categorical_features)),\n",
    "    \"heavy_threshold\": float(HEAVY_THRESH),\n",
    "    \"target\": str(TARGET)\n",
    "}\n",
    "with open(ARTIFACT_DIR / \"features.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "# 3) save training & evaluation metadata (fill with values you reported)\n",
    "eval_metrics = {\n",
    "    \"data\": {\n",
    "        \"initial_shape\": [275272, 37],\n",
    "        \"after_filtering\": [267329, 37],\n",
    "        \"train_shape\": [213863, 37],\n",
    "        \"test_shape\": [53466, 37],\n",
    "        \"rolling_train_cols\": 53,\n",
    "        \"numeric_features_used\": len(numeric_features),\n",
    "        \"categorical_features_used\": len(categorical_features),\n",
    "        \"normal_train_samples\": int((train_df[TARGET] <= HEAVY_THRESH).sum()) if 'train_df' in globals() else None,\n",
    "        \"heavy_train_samples\": int((train_df[TARGET] > HEAVY_THRESH).sum()) if 'train_df' in globals() else None\n",
    "    },\n",
    "    \"classifier\": {\n",
    "        \"accuracy\": 0.927,\n",
    "        \"f1_heavy\": 0.698,\n",
    "        \"precision_heavy\": 0.874,\n",
    "        \"recall_heavy\": 0.582,\n",
    "        \"confusion_matrix\": [[45058, 653], [3245, 4510]]\n",
    "    },\n",
    "    \"regression\": {\n",
    "        \"RMSE\": 38.226,\n",
    "        \"MAE\": 16.313,\n",
    "        \"R2\": 0.604,\n",
    "        \"normal_segment\": {\"MAE\": 12.543, \"RMSE\": 15.552},\n",
    "        \"heavy_segment\": {\"MAE\": 38.538, \"RMSE\": 92.999, \"R2\": 0.406}\n",
    "    }\n",
    "}\n",
    "with open(ARTIFACT_DIR / \"metadata.json\", \"w\") as f:\n",
    "    json.dump(eval_metrics, f, indent=2)\n",
    "\n",
    "# 4) save runtime & library versions\n",
    "versions = {\n",
    "    \"python\": sys.version.splitlines()[0],\n",
    "    \"sklearn\": sklearn.__version__,\n",
    "    \"joblib\": getattr(sys.modules.get('joblib'), '__version__', None),\n",
    "}\n",
    "try:\n",
    "    import xgboost\n",
    "    versions[\"xgboost\"] = xgboost.__version__\n",
    "except Exception:\n",
    "    versions[\"xgboost\"] = None\n",
    "\n",
    "with open(ARTIFACT_DIR / \"versions.json\", \"w\") as f:\n",
    "    json.dump(versions, f, indent=2)\n",
    "\n",
    "# 5) create a tiny inference helper module (predictor.py) saved into artifacts\n",
    "print(\"Creating predictor helper module...\")\n",
    "predictor_code = f'''\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "from pathlib import Path\n",
    "\n",
    "ART_DIR = Path(\"{ARTIFACT_DIR.as_posix()}\")\n",
    "\n",
    "# load artifacts\n",
    "clf = load(ART_DIR / \"clf_pipe.joblib\")\n",
    "reg_norm = load(ART_DIR / \"normal_reg_pipe.joblib\")\n",
    "reg_heavy = load(ART_DIR / \"heavy_reg_pipe.joblib\")\n",
    "with open(ART_DIR / \"features.json\", \"r\") as f:\n",
    "    features = json.load(f)\n",
    "\n",
    "HEAVY_THRESH = {HEAVY_THRESH}\n",
    "\n",
    "def predict_single(row_dict):\n",
    "    \"\"\"\n",
    "    row_dict: mapping of feature_name -> value (must include categorical keys and numeric keys used by the model)\n",
    "    Returns: dict with predicted_delay (minutes) and heavy_flag (0/1)\n",
    "    \"\"\"\n",
    "    # build DataFrame\n",
    "    df = pd.DataFrame([row_dict])\n",
    "    X = df[features['numeric_features'] + features['categorical_features']]\n",
    "\n",
    "    heavy_prob = clf.predict(X)[0]\n",
    "    # regressors expect the whole feature set; they output log1p predictions\n",
    "    pred_norm_log = reg_norm.predict(X)[0]\n",
    "    pred_heavy_log = reg_heavy.predict(X)[0]\n",
    "    pred_norm = float(np.expm1(pred_norm_log))\n",
    "    pred_heavy = float(np.expm1(pred_heavy_log))\n",
    "    pred = pred_heavy if heavy_prob == 1 else pred_norm\n",
    "    pred = max(0.0, pred)\n",
    "    return {{\n",
    "        \"predicted_delay_min\": pred,\n",
    "        \"heavy_flag\": int(heavy_prob),\n",
    "        \"pred_norm\": pred_norm,\n",
    "        \"pred_heavy\": pred_heavy\n",
    "    }}\n",
    "'''\n",
    "\n",
    "with open(ARTIFACT_DIR / \"predictor.py\", \"w\") as f:\n",
    "    f.write(predictor_code)\n",
    "\n",
    "# 6) Small smoke test (if test_df available)\n",
    "smoke = {}\n",
    "if 'test_df' in globals():\n",
    "    try:\n",
    "        sample_row = X_test_full.iloc[0].to_dict()\n",
    "        from importlib import import_module\n",
    "        spec = __import__(f\"model_artifacts.predictor\", fromlist=['predict_single'])\n",
    "    except Exception:\n",
    "        # try calling the saved module directly by loading predictor.py\n",
    "        import importlib.util, importlib.machinery\n",
    "        spec = importlib.util.spec_from_file_location(\"predictor_mod\", str(ARTIFACT_DIR / \"predictor.py\"))\n",
    "        mod = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(mod)\n",
    "        smoke = mod.predict_single(X_test_full.iloc[0].to_dict())\n",
    "print(\"Saved artifacts to:\", ARTIFACT_DIR)\n",
    "print(\"Artifacts list:\", list(ARTIFACT_DIR.iterdir()))\n",
    "print(\"Smoke test prediction (example):\", smoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98143efa-032c-40be-8507-5b98fb09a079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
